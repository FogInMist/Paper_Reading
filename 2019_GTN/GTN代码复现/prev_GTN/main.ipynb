{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTN官方代码测试及说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import GTN\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "from utils import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='DBLP', epoch=40, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=2, norm='true', adaptive_lr='false')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='DBLP',\n",
    "                    help='Dataset')\n",
    "parser.add_argument('--epoch', type=int, default=40,\n",
    "                    help='Training Epochs')\n",
    "parser.add_argument('--node_dim', type=int, default=64,\n",
    "                    help='Node dimension')\n",
    "parser.add_argument('--num_channels', type=int, default=2,\n",
    "                    help='number of channels')\n",
    "parser.add_argument('--lr', type=float, default=0.005,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001,\n",
    "                    help='l2 reg')\n",
    "parser.add_argument('--num_layers', type=int, default=2,\n",
    "                    help='number of layer')\n",
    "parser.add_argument('--norm', type=str, default='true',\n",
    "                    help='normalization')\n",
    "parser.add_argument('--adaptive_lr', type=str, default='false',\n",
    "                    help='adaptive learning rate')\n",
    "\n",
    "#args = parser.parse_args()                                               # pychram 中使用\n",
    "args = parser.parse_known_args()[0]                                       # jupyter 中使用\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = args.epoch\n",
    "node_dim = args.node_dim\n",
    "num_channels = args.num_channels\n",
    "lr = args.lr\n",
    "weight_decay = args.weight_decay\n",
    "num_layers = args.num_layers\n",
    "norm = args.norm\n",
    "adaptive_lr = args.adaptive_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "with open('../data/'+args.dataset+'/node_features.pkl','rb') as f:\n",
    "    node_features = pickle.load(f)\n",
    "with open('../data/'+args.dataset+'/edges.pkl','rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "with open('../data/'+args.dataset+'/labels.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "num_nodes = edges[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18405, 334)\n",
      "(18405, 18405)\n",
      "3\n",
      "[[0, 1], [1, 3], [2, 0], [3, 3], [4, 0], [5, 0], [6, 3], [7, 3], [8, 2], [9, 0], [10, 2], [11, 0], [12, 0], [13, 0], [14, 1], [15, 2], [16, 3], [17, 0], [18, 2], [19, 3], [20, 0], [21, 1], [22, 2], [23, 2], [24, 2], [25, 2], [26, 2], [27, 2], [28, 0], [29, 1], [30, 0], [31, 0], [32, 1], [33, 2], [34, 2], [35, 3], [36, 3], [37, 2], [38, 3], [39, 0], [40, 3], [41, 2], [42, 1], [43, 2], [44, 1], [45, 0], [46, 0], [47, 1], [48, 0], [49, 1], [50, 1], [51, 2], [52, 3], [53, 3], [54, 2], [55, 2], [56, 3], [57, 1], [58, 3], [59, 2], [60, 0], [61, 1], [62, 1], [63, 3], [64, 3], [65, 2], [66, 2], [67, 2], [68, 1], [69, 1], [70, 2], [71, 3], [72, 0], [73, 0], [74, 0], [75, 1], [76, 0], [77, 3], [78, 0], [79, 0], [80, 1], [81, 0], [82, 1], [83, 2], [84, 2], [85, 3], [86, 0], [87, 3], [88, 0], [89, 0], [90, 2], [91, 3], [92, 2], [93, 0], [94, 3], [95, 0], [96, 0], [97, 0], [98, 0], [99, 3], [100, 2], [101, 0], [102, 2], [103, 3], [104, 0], [105, 2], [106, 2], [107, 3], [108, 2], [109, 2], [110, 2], [111, 0], [112, 2], [113, 2], [114, 0], [115, 3], [116, 3], [117, 2], [118, 2], [119, 1], [120, 2], [121, 0], [122, 0], [123, 2], [124, 2], [125, 0], [126, 0], [127, 2], [128, 0], [129, 3], [130, 3], [131, 3], [132, 3], [133, 2], [134, 0], [135, 2], [136, 1], [137, 0], [138, 2], [139, 2], [140, 2], [141, 2], [142, 2], [143, 2], [144, 2], [145, 0], [146, 2], [147, 3], [148, 1], [149, 2], [150, 3], [151, 2], [152, 1], [153, 2], [154, 0], [155, 0], [156, 0], [157, 0], [158, 0], [159, 2], [160, 3], [161, 0], [162, 3], [163, 0], [164, 1], [165, 0], [166, 1], [167, 2], [168, 0], [169, 3], [170, 0], [171, 2], [172, 2], [173, 0], [174, 3], [175, 0], [176, 1], [177, 1], [178, 3], [179, 1], [180, 2], [181, 0], [182, 2], [183, 2], [184, 1], [185, 0], [186, 2], [187, 2], [188, 0], [189, 3], [190, 2], [191, 0], [192, 3], [193, 2], [194, 0], [195, 0], [196, 3], [197, 2], [198, 2], [199, 3], [200, 0], [201, 0], [202, 2], [203, 3], [204, 1], [205, 2], [206, 1], [207, 2], [208, 2], [209, 0], [210, 0], [211, 2], [212, 3], [213, 0], [214, 2], [215, 0], [216, 0], [217, 3], [218, 2], [219, 0], [220, 2], [221, 0], [222, 0], [223, 1], [224, 3], [225, 2], [226, 3], [227, 1], [228, 0], [229, 2], [230, 1], [231, 3], [232, 0], [233, 3], [234, 3], [235, 2], [236, 3], [237, 2], [238, 2], [239, 1], [240, 3], [241, 1], [242, 2], [243, 0], [244, 1], [245, 3], [246, 0], [247, 3], [248, 3], [249, 0], [250, 2], [251, 2], [252, 0], [253, 2], [254, 0], [255, 1], [256, 3], [257, 0], [258, 0], [259, 3], [260, 0], [261, 2], [262, 2], [263, 1], [264, 1], [265, 2], [266, 3], [267, 1], [268, 0], [269, 1], [270, 2], [271, 0], [272, 3], [273, 0], [274, 2], [275, 3], [276, 3], [277, 0], [278, 3], [279, 1], [280, 3], [281, 1], [282, 3], [283, 2], [284, 2], [285, 0], [286, 0], [287, 2], [288, 1], [289, 2], [290, 3], [291, 3], [292, 1], [293, 3], [294, 2], [295, 2], [296, 1], [297, 3], [298, 2], [299, 3], [300, 0], [301, 2], [302, 0], [303, 0], [304, 0], [305, 3], [306, 0], [307, 2], [308, 2], [309, 1], [310, 3], [311, 1], [312, 2], [313, 0], [314, 1], [315, 0], [316, 3], [317, 3], [318, 2], [319, 3], [320, 3], [321, 3], [322, 0], [323, 0], [324, 0], [325, 3], [326, 1], [327, 2], [328, 1], [329, 2], [330, 0], [331, 3], [332, 0], [333, 0], [334, 3], [335, 0], [336, 0], [337, 0], [338, 2], [339, 2], [340, 2], [341, 3], [342, 0], [343, 3], [344, 2], [345, 1], [346, 2], [347, 1], [348, 2], [349, 2], [350, 2], [351, 3], [352, 3], [353, 2], [354, 2], [355, 2], [356, 1], [357, 3], [358, 0], [359, 3], [360, 2], [361, 0], [362, 2], [363, 0], [364, 0], [365, 0], [366, 2], [367, 2], [368, 3], [369, 0], [370, 3], [371, 0], [372, 0], [373, 2], [374, 1], [375, 0], [376, 2], [377, 0], [378, 0], [379, 0], [380, 2], [381, 2], [382, 0], [383, 2], [384, 0], [385, 2], [386, 1], [387, 2], [388, 2], [389, 3], [390, 1], [391, 0], [392, 1], [393, 2], [394, 2], [395, 0], [396, 2], [397, 1], [398, 0], [399, 2], [400, 2], [401, 1], [402, 2], [403, 2], [404, 3], [405, 0], [406, 3], [407, 2], [408, 3], [409, 1], [410, 0], [411, 2], [412, 2], [413, 2], [414, 3], [415, 2], [416, 1], [417, 3], [418, 0], [419, 3], [420, 3], [421, 3], [422, 0], [423, 2], [424, 2], [425, 2], [426, 3], [427, 1], [428, 2], [429, 2], [430, 3], [431, 3], [432, 1], [433, 3], [434, 0], [435, 2], [436, 0], [437, 0], [438, 2], [439, 0], [440, 3], [441, 3], [442, 2], [443, 1], [444, 0], [445, 0], [446, 0], [447, 0], [448, 0], [449, 2], [450, 3], [451, 0], [452, 0], [453, 0], [454, 0], [455, 1], [456, 3], [457, 2], [458, 0], [459, 1], [460, 0], [461, 2], [462, 0], [463, 2], [464, 3], [465, 2], [466, 2], [467, 2], [468, 2], [469, 2], [470, 2], [471, 0], [472, 2], [473, 0], [474, 2], [475, 0], [476, 0], [477, 0], [478, 0], [479, 0], [480, 0], [481, 2], [482, 2], [483, 2], [484, 2], [485, 3], [486, 0], [487, 3], [488, 1], [489, 1], [490, 3], [491, 2], [492, 0], [493, 0], [494, 3], [495, 0], [496, 0], [497, 2], [498, 2], [499, 3], [500, 3], [501, 0], [502, 0], [503, 2], [504, 0], [505, 0], [506, 0], [507, 1], [508, 0], [509, 3], [510, 0], [511, 0], [512, 0], [513, 1], [514, 3], [515, 2], [516, 0], [517, 2], [518, 3], [519, 1], [520, 0], [521, 3], [522, 0], [523, 2], [524, 0], [525, 2], [526, 0], [527, 2], [528, 0], [529, 1], [530, 0], [531, 3], [532, 3], [533, 1], [534, 0], [535, 2], [536, 3], [537, 3], [538, 1], [539, 2], [540, 1], [541, 0], [542, 0], [543, 0], [544, 2], [545, 2], [546, 0], [547, 0], [548, 2], [549, 2], [550, 1], [551, 2], [552, 3], [553, 2], [554, 3], [555, 0], [556, 2], [557, 0], [558, 1], [559, 0], [560, 0], [561, 3], [562, 3], [563, 0], [564, 2], [565, 0], [566, 2], [567, 3], [568, 1], [569, 3], [570, 2], [571, 2], [572, 3], [573, 0], [574, 2], [575, 3], [576, 3], [577, 0], [578, 2], [579, 2], [580, 2], [581, 2], [582, 3], [583, 0], [584, 1], [585, 0], [586, 0], [587, 0], [588, 3], [589, 3], [590, 1], [591, 1], [592, 2], [593, 2], [594, 2], [595, 0], [596, 3], [597, 1], [598, 0], [599, 3], [600, 2], [601, 0], [602, 3], [603, 2], [604, 3], [605, 3], [606, 2], [607, 2], [608, 0], [609, 0], [610, 3], [611, 2], [612, 2], [613, 2], [614, 0], [615, 0], [616, 1], [617, 3], [618, 3], [619, 0], [620, 1], [621, 0], [622, 1], [623, 1], [624, 0], [625, 3], [626, 0], [627, 3], [628, 3], [629, 3], [630, 1], [631, 1], [632, 3], [633, 1], [634, 3], [635, 3], [636, 1], [637, 3], [638, 1], [639, 3], [640, 3], [641, 3], [642, 1], [643, 3], [644, 3], [645, 3], [646, 3], [647, 1], [648, 1], [649, 3], [650, 3], [651, 1], [652, 1], [653, 1], [654, 3], [655, 3], [656, 3], [657, 1], [658, 1], [659, 3], [660, 3], [661, 3], [662, 3], [663, 1], [664, 3], [665, 3], [666, 3], [667, 3], [668, 1], [669, 1], [670, 3], [671, 3], [672, 1], [673, 1], [674, 1], [675, 1], [676, 3], [677, 3], [678, 1], [679, 1], [680, 1], [681, 3], [682, 1], [683, 1], [684, 1], [685, 3], [686, 3], [687, 3], [688, 1], [689, 1], [690, 1], [691, 3], [692, 1], [693, 3], [694, 1], [695, 1], [696, 3], [697, 3], [698, 3], [699, 1], [700, 1], [701, 3], [702, 3], [703, 3], [704, 3], [705, 1], [706, 1], [707, 3], [708, 3], [709, 1], [710, 1], [711, 1], [712, 3], [713, 1], [714, 3], [715, 1], [716, 1], [717, 3], [718, 3], [719, 1], [720, 3], [721, 3], [722, 3], [723, 1], [724, 3], [725, 3], [726, 3], [727, 1], [728, 1], [729, 1], [730, 1], [731, 3], [732, 3], [733, 1], [734, 1], [735, 3], [736, 3], [737, 1], [738, 1], [739, 1], [740, 1], [741, 1], [742, 1], [743, 1], [744, 1], [745, 1], [746, 1], [747, 1], [748, 1], [749, 1], [750, 1], [751, 1], [752, 1], [753, 1], [754, 1], [755, 1], [756, 1], [757, 1], [758, 1], [759, 1], [760, 1], [761, 1], [762, 1], [763, 1], [764, 1], [765, 1], [766, 1], [767, 1], [768, 1], [769, 1], [770, 1], [771, 1], [772, 1], [773, 1], [774, 1], [775, 1], [776, 1], [777, 1], [778, 1], [779, 1], [780, 1], [781, 1], [782, 1], [783, 1], [784, 1], [785, 1], [786, 1], [787, 1], [788, 1], [789, 1], [790, 1], [791, 1], [792, 1], [793, 1], [794, 1], [795, 1], [796, 1], [797, 1], [798, 1], [799, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(node_features.shape)\n",
    "print(edges[0].shape)\n",
    "print(len(labels))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造邻接矩阵\n",
    "\n",
    "**对图的邻接矩阵进行处理，将其转换为PyTorch张量，并将其拼接在一起:**\n",
    "\n",
    "1. 遍历edges中的每个邻接矩阵edge,其中i表示当前遍历的邻接矩阵索引。\n",
    "2. 如果i为0，则将edge转换为PyTorch张量，并将其unsqueeze到最后一维以便在之后进行拼接。\n",
    "3. 如果i不为0，则将当前邻接矩阵edge转换为PyTorch张量，并将其unsqueeze到最后一维，然后将结果与之前的邻接矩阵A进行cat操作，将它们拼接在一起。\n",
    "4. 最后，将torch.eye(num nodes)(一个单位矩阵)转换为PyTorch张量，并将其unsqueeze到最后一维，然后将结果与A进行cat操作，将它们拼接在一起。这样就得到了最终的邻接矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,edge in enumerate(edges):\n",
    "    if i ==0:\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        A = torch.cat([A,torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "A = torch.cat([A,torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18405, 18405, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练集、验证集和测试集数据转化为LongTensor格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor)\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor)\n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor)\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor)\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor)\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18405, 334])\n",
      "torch.Size([800])\n",
      "torch.Size([800])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([2857])\n",
      "torch.Size([2857])\n",
      "torch.Size([18405, 18405, 5])\n"
     ]
    }
   ],
   "source": [
    "print(node_features.shape)\n",
    "print(train_node.shape)\n",
    "print(train_target.shape)\n",
    "print(valid_node.shape)\n",
    "print(valid_target.shape)\n",
    "print(test_node.shape)\n",
    "print(test_target.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终的类别数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = torch.max(train_target).item()+1\n",
    "final_f1 = 0\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1\n",
      "Train - Loss: 1.390804409980774, Macro_F1: 0.11158082634210587\n",
      "Valid - Loss: 1.3697787523269653, Macro_F1: 0.2811485528945923\n",
      "Test - Loss: 1.3840534687042236, Macro_F1: 0.24715301394462585\n",
      "\n",
      "---------------Best Results--------------------\n",
      "Train - Loss: 1.390804409980774, Macro_F1: 0.11158082634210587\n",
      "Valid - Loss: 1.3697787523269653, Macro_F1: 0.2811485528945923\n",
      "Test - Loss: 1.3840534687042236, Macro_F1: 0.24715301394462585\n"
     ]
    }
   ],
   "source": [
    "for l in range(1):\n",
    "    model = GTN(num_edge=A.shape[-1], # 图中不同类型的边的类别数，从邻接矩阵A的最后一维中获取\n",
    "                        num_channels=num_channels, # 图卷积层的通道数\n",
    "                        w_in = node_features.shape[1], # 输入特征的维度，从节点特征node_features.shape[1]中获取\n",
    "                        w_out = node_dim, # 隐藏层输出的特征维度\n",
    "                        num_class=num_classes, # 分类任务中的类别数\n",
    "                        num_layers=num_layers, # 图卷积层的数量\n",
    "                        norm=norm) # 是否对邻接矩阵进行归一化\n",
    "    \n",
    "    if adaptive_lr == 'false': # 学习率是否为自适应\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam([{'params':model.weight},\n",
    "                                    {'params':model.linear1.parameters()},\n",
    "                                    {'params':model.linear2.parameters()},\n",
    "                                    {\"params\":model.layers.parameters(), \"lr\":0.5}\n",
    "                                    ], lr=0.005, weight_decay=0.001)\n",
    "    loss = nn.CrossEntropyLoss() # 用于多分类任务的标准交叉熵损失函数\n",
    "\n",
    "    # Train & Valid & Test\n",
    "    best_val_loss = 10000\n",
    "    best_test_loss = 10000\n",
    "    best_train_loss = 10000\n",
    "    best_train_f1 = 0\n",
    "    best_val_f1 = 0\n",
    "    best_test_f1 = 0\n",
    "    \n",
    "    # for i in range(epochs):\n",
    "    for i in range(1):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            if param_group['lr'] > 0.005:\n",
    "                param_group['lr'] = param_group['lr'] * 0.9  # 学习率衰减\n",
    "        print('Epoch:  ',i+1)\n",
    "\n",
    "        model.zero_grad()   # 梯度归零\n",
    "        model.train()       # 声明模型训练\n",
    "        loss,y_train,Ws = model(A, node_features, train_node, train_target) # 损失值和结果\n",
    "        train_f1 = torch.mean(f1_score(torch.argmax(y_train.detach(),dim=1), train_target, num_classes=num_classes)).cpu().numpy() # 训练集的F1得分\n",
    "        print('Train - Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 梯度更新\n",
    "\n",
    "        # Valid\n",
    "        model.eval() # 声明模型测试\n",
    "        with torch.no_grad(): # 无梯度声明\n",
    "            val_loss, y_valid,_ = model.forward(A, node_features, valid_node, valid_target)\n",
    "            val_f1 = torch.mean(f1_score(torch.argmax(y_valid,dim=1), valid_target, num_classes=num_classes)).cpu().numpy() # 验证集的F1得分\n",
    "            print('Valid - Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "            test_loss, y_test,W = model.forward(A, node_features, test_node, test_target)\n",
    "            test_f1 = torch.mean(f1_score(torch.argmax(y_test,dim=1), test_target, num_classes=num_classes)).cpu().numpy() # 测试集的F1得分\n",
    "            print('Test - Loss: {}, Macro_F1: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1))\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_loss = val_loss.detach().cpu().numpy()\n",
    "            best_test_loss = test_loss.detach().cpu().numpy()\n",
    "            best_train_loss = loss.detach().cpu().numpy()\n",
    "            best_train_f1 = train_f1\n",
    "            best_val_f1 = val_f1\n",
    "            best_test_f1 = test_f1 \n",
    "            \n",
    "    print('---------------Best Results--------------------')\n",
    "    print('Train - Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "    print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "    print('Test - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_test_f1))\n",
    "    final_f1 += best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
